# Health-InfraOps Ceph Storage Configuration

[global]
fsid = a7f64266-7c70-42a5-bc7e-4f5e9e2a8c1e
mon initial members = mon01, mon02, mon03
mon host = 10.0.50.11, 10.0.50.12, 10.0.50.13
public network = 10.0.50.0/24
cluster network = 10.0.50.0/24
auth cluster required = cephx
auth service required = cephx
auth client required = cephx
osd journal size = 1024
osd pool default size = 3
osd pool default min size = 2
osd pool default pg num = 128
osd pool default pgp num = 128
osd crush chooseleaf type = 1

# Monitoring
[mon]
mon data = /var/lib/ceph/mon/ceph-$id
mon clock drift allowed = 1
mon osd min down reporters = 2
mon osd down out interval = 600

# Manager
[mgr]
mgr modules = dashboard, restful

# OSD Configuration
[osd]
osd data = /var/lib/ceph/osd/ceph-$id
osd journal size = 1024
osd journal = /var/lib/ceph/osd/ceph-$id/journal
osd mkfs type = xfs
osd mkfs options xfs = -f -i size=2048
osd mount options xfs = noatime,largeio,inode64,swalloc
osd recovery max active = 3
osd max backfills = 1
osd op threads = 4
osd disk threads = 2
osd map cache size = 1024
osd map cache bl size = 128
osd pool default erasure code profile = plugin=jerasure technique=reed_sol_van k=2 m=1

# Client Configuration
[client]
rbd cache = true
rbd cache size = 64M
rbd cache max dirty = 32M
rbd cache target dirty = 24M
rbd cache max dirty age = 2

# Rados Gateway
[client.rgw.health-infraops]
rgw frontends = civetweb port=10.0.50.11:8080
rgw dns name = s3.infokes.co.id
rgw print continue = false
rgw cache enabled = true
rgw cache lru size = 10000
rgw socket path = /var/run/ceph/ceph-client.rgw.health-infraops.asok
rgw data = /var/lib/ceph/radosgw/ceph-rgw.health-infraops